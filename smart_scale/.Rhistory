labs(x='Calculated Smart Scale Score', y='Assigned Smart Scale Score',
title='Fig. 4: Calculated vs. Assigned Smart Scale Scores')
# Plot for Area Type = D
ggplot(data=subset(data, Area.Type == 'D'),
aes(x=Calc.SMART.SCALE.Score, y=SMART.SCALE.Score))+#, color=Area.Type, shape=Area.Type)) +
geom_point() +
geom_smooth(method=lm) +
labs(x='Calculated Smart Scale Score', y='Assigned Smart Scale Score',
title='Fig. 4: Calculated vs. Assigned Smart Scale Scores')
ggarrange
ggarrange
multiplot
ggplot
library(ggplot2)
multiplot
# Full Plot
ggplot(data=data,
aes(x=SMART.SCALE.Score, y=Calc.SMART.SCALE.Score))+#, color=Area.Type, shape=Area.Type)) +
geom_point() +
geom_smooth(method=lm) +
labs(x='Calculated Smart Scale Score', y='Assigned Smart Scale Score',
title='Fig. 4: Calculated vs. Assigned Smart Scale Scores')
gtid.arrange
library(gridextra)
library(gridExtra)
ss.scatter.plot <- function(dataset, title='Actual vs. Assigned Smart Scale Scores') {
ggplot(data=dataset,
aes(x=SMART.SCALE.Score, y=Calc.SMART.SCALE.Score)) +
geom_point() +
geom_smooth(method=lm) +
labs(x='Calculated Smart Scale Score', y='Assigned Smart Scale Score',
title=title)
}
sp.a <- ss.scatter.plot(subset(data, Area.Type == 'A'), 'Area Type = A')
sp.b <- ss.scatter.plot(subset(data, Area.Type == 'B'), 'Area Type = B')
sp.c <- ss.scatter.plot(subset(data, Area.Type == 'C'), 'Area Type = C')
sp.d <- ss.scatter.plot(subset(data, Area.Type == 'D'), 'Area Type = D')
grid.arrange(sp.a, sp.b, sp.c, sp.d, nrow=2)
library(gridExtra)
install.packages('gridExtra')
library(gridExtra)
sp.a <- ss.scatter.plot(subset(data, Area.Type == 'A'), 'Area Type = A')
sp.b <- ss.scatter.plot(subset(data, Area.Type == 'B'), 'Area Type = B')
sp.c <- ss.scatter.plot(subset(data, Area.Type == 'C'), 'Area Type = C')
sp.d <- ss.scatter.plot(subset(data, Area.Type == 'D'), 'Area Type = D')
grid.arrange(sp.a, sp.b, sp.c, sp.d, nrow=2)
ss.scatter.plot <- function(dataset, title='Actual vs. Assigned Smart Scale Scores') {
ggplot(data=dataset,
aes(x=Calc.SMART.SCALE.Score, y=SMART.SCALE.Score)) +
geom_point() +
geom_smooth(method=lm) +
labs(x='Calculated Smart Scale Score', y='Assigned Smart Scale Score',
title=title)
}
sp.a <- ss.scatter.plot(subset(data, Area.Type == 'A'), 'Area Type = A')
sp.b <- ss.scatter.plot(subset(data, Area.Type == 'B'), 'Area Type = B')
sp.c <- ss.scatter.plot(subset(data, Area.Type == 'C'), 'Area Type = C')
sp.d <- ss.scatter.plot(subset(data, Area.Type == 'D'), 'Area Type = D')
grid.arrange(sp.a, sp.b, sp.c, sp.d, nrow=2) +
ggtitle('Assigned vs. Calculated Smart Scale Scores')
#------ Step 4: Apply Factor Weights
data['Calc.SMART.SCALE.Score'] <- 1e7 * data[['Calc.Project.Benefit.Score']] / data[['SMART.SCALE.Request']]
ss.scatter.plot <- function(dataset, title='Actual vs. Assigned Smart Scale Scores') {
ggplot(data=dataset,
aes(x=Calc.SMART.SCALE.Score, y=SMART.SCALE.Score)) +
geom_point() +
geom_smooth(method=lm) +
labs(x='Calculated Smart Scale Score', y='Assigned Smart Scale Score',
title=title)
}
sp.a <- ss.scatter.plot(subset(data, Area.Type == 'A'), 'Area Type A')
sp.b <- ss.scatter.plot(subset(data, Area.Type == 'B'), 'Area Type B')
sp.c <- ss.scatter.plot(subset(data, Area.Type == 'C'), 'Area Type C')
sp.d <- ss.scatter.plot(subset(data, Area.Type == 'D'), 'Area Type D')
grid.arrange(sp.a, sp.b, sp.c, sp.d, nrow=2)
library(restriktor) # Used for the constrained regression.
library(ggplot2)
library(gridExtra)
data <- read.csv('ss-data.csv')
# --------------- Part 1: Data Preprocessing --------------------------
recode <- function(vec, from.vals, to.vals) {
f = function(v) {
for(i in 1:length(from.vals)) {
if(v == from.vals[i]) { return(to.vals[i]) }
}
return(v)
}
return(sapply(as.vector(vec), f))
}
# Function for imputing mean of a vector to its missing values.
impute.mean <- function(vec) {
vec[is.na(vec)] = mean(vec, na.rm=TRUE)
vec
}
# Function for imputing value of 0 to each missing value in the vector.
impute.0 <- function(vec) {
vec[is.na(vec)] = 0
vec
}
# Clean and recode numeric columns.
numeric.columns <- colnames(data)[8:ncol(data)]
for(col in numeric.columns) {
data[[col]] <- as.numeric(as.character(data[[col]]))
}
imputer.f <- impute.0 # Missing value imputation function - can change if needed
# Impute missing values to component scores and print out the percent missing in each column.
for(i in 8:ncol(data)) {
message(paste('Percent Missing Values for Column ', colnames(data)[i], ': ',
round(100*(1 - (length(sort(data[[i]]))/nrow(data))), 2)))
data[[i]] <- imputer.f(data[[i]])
}
# Properly binarize binary columns.
data$Statewide.High.Priority <- as.numeric(sapply(data$Statewide.High.Priority, function(x){if(x == 'x') return(1); return(0);}))
data$District.Grant <- as.numeric(sapply(data$District.Grant, function(x){if(x == 'x') return(1); return(0);}))
# --------------- Part 3: Investigating Smart Scale Calculation --------------------------
# Look at Smart Scale Score fit by area type.
# In this section we use regression to calculate the actual weights of each factor by area type. These
# can be then compared to the stated weights in the technical guide to see if they are in fact consistent.
#------ Step 1: Normalization of Measure Weights
measure.cols <- c('Throughput.Score', 'Delay.Score',
'Econ.Dev.Support.Score', 'Intermodal.Access.Score', 'Travel.Time.Reliability.Score',
'Access.to.Jobs', 'Disadvantaged.Access.to.Jobs', 'Multimodal.Access.Score',
'Crash.Frequency.Score', 'Crash.Rate.Score',
'Air.Quality.Score', 'Enviro.Impact.Score')
for(i in measure.cols) {data[[i]] <- 100 * data[[i]] / max(data[[i]])}
#------ Step 2: Apply Measure Weights
# For a given data frame, a set of weights, and a set of selected columns, this function returns a new
# vector corresponding to the weighted averages of the columns.
weighted.column.average <- function(data.frame, weight.vec, selected.columns) {
v <- 0
for(i in 1:length(weight.vec)) {
v <- v + (weight.vec[i] * data.frame[[selected.columns[i]]])
}
v
}
# Calculate the composite factor scores (except Land Use, which is APPARENTLY already given in the data) from
# component scores according to the Smart Scale November 2017 technical guide, p. 40.
data$Congestion.Score <- weighted.column.average(data, c(0.5, 0.5), c('Throughput.Score', 'Delay.Score'))
data$Economic.Score <- weighted.column.average(data, c(0.6, 0.2, 0.2), c('Econ.Dev.Support.Score', 'Intermodal.Access.Score', 'Travel.Time.Reliability.Score'))
data$Accessibility.Score <- weighted.column.average(data, c(0.6, 0.2, 0.2), c('Access.to.Jobs', 'Disadvantaged.Access.to.Jobs', 'Multimodal.Access.Score'))
data$Safety.Score <- weighted.column.average(data, c(0.5, 0.5), c('Crash.Frequency.Score', 'Crash.Rate.Score'))
data$Environmental.Score <- weighted.column.average(data, c(0.5, 0.5), c('Air.Quality.Score', 'Enviro.Impact.Score'))
# Land use score already in data - no component scores listed.
#------ Step 3: Apply Factor Weights
# Given a dataframe with calculated factor scores, this function computes the project value according to
# Table 4.2, p.36 of the technical guide.
calc.proj.value <- function(dataset) {
factor.cols.ab <- c('Congestion.Score', 'Economic.Score', 'Accessibility.Score', 'Safety.Score',
'Environmental.Score', 'Land.Use.Score')
factor.cols.cd <- c('Congestion.Score', 'Economic.Score', 'Accessibility.Score', 'Safety.Score',
'Environmental.Score')
sa <- weighted.column.average(dataset, c(0.45, 0.5, 0.15, 0.5, 0.1, 0.2), factor.cols.ab)
sb <- weighted.column.average(dataset, c(0.15, 0.2, 0.25, 0.2, 0.1, 0.1), factor.cols.ab)
sc <- weighted.column.average(dataset, c(0.15, 0.25, 0.25, 0.25, 0.1), factor.cols.cd)
sd <- weighted.column.average(dataset, c(0.10, 0.35, 0.15, 0.3, 0.1), factor.cols.cd)
scores <- c()
for(i in 1:nrow(dataset)) {
if(dataset[i, 'Area.Type'] == 'A') {scores[i] <- sa[i]}
else if(dataset[i, 'Area.Type'] == 'B') {scores[i] <- sb[i]}
else if(dataset[i, 'Area.Type'] == 'C') {scores[i] <- sc[i]}
else {scores[i] <- sd[i]}
}
scores
}
# Add a new column to data with our calculated project benefit scores.
data['Calc.Project.Benefit.Score'] <- calc.proj.value(data)
data[['Calc.Project.Benefit.Score']]
data[['Project.Benefit.Score']]
#------ Step 4: Apply Factor Weights
data['Calc.SMART.SCALE.Score'] <- 1e7 * data[['Calc.Project.Benefit.Score']] / data[['SMART.SCALE.Request']]
data[['Calc.SMART.SCALE.Score']][1:10]
data[['SMART.SCALE.Score']][1:10]
ss.scatter.plot <- function(dataset, title='Actual vs. Assigned Smart Scale Scores') {
ggplot(data=dataset,
aes(x=Calc.SMART.SCALE.Score, y=SMART.SCALE.Score)) +
geom_point() +
geom_smooth(method=lm) +
labs(x='Calculated Smart Scale Score', y='Assigned Smart Scale Score',
title=title)
}
summary(lm(SMART.SCALE.Score ~ Calc.SMART.SCALE.Score, data=subset(data, Area.Type == 'A')))
summary(lm(SMART.SCALE.Score ~ Calc.SMART.SCALE.Score, data=subset(data, Area.Type == 'B')))
summary(lm(SMART.SCALE.Score ~ Calc.SMART.SCALE.Score, data=subset(data, Area.Type == 'C')))
summary(lm(SMART.SCALE.Score ~ Calc.SMART.SCALE.Score, data=subset(data, Area.Type == 'D')))
coefficients(lm(SMART.SCALE.Score ~ Calc.SMART.SCALE.Score, data=subset(data, Area.Type == 'D')))
r2(lm(SMART.SCALE.Score ~ Calc.SMART.SCALE.Score, data=subset(data, Area.Type == 'D')))
str(lm(SMART.SCALE.Score ~ Calc.SMART.SCALE.Score, data=subset(data, Area.Type == 'D')))$adj.r.squared
str(lm(SMART.SCALE.Score ~ Calc.SMART.SCALE.Score, data=subset(data, Area.Type == 'D')))
str(summary(lm(SMART.SCALE.Score ~ Calc.SMART.SCALE.Score, data=subset(data, Area.Type == 'D'))))
str(summary(lm(SMART.SCALE.Score ~ Calc.SMART.SCALE.Score, data=subset(data, Area.Type == 'D'))))$adj.r.squared
d.lm <- lm(SMART.SCALE.Score ~ Calc.SMART.SCALE.Score, data=subset(data, Area.Type == 'D'))
str(summary(d.lm))$adj.r.squared
summary(d.lm)$adj.r.squared
paste(c(1,2,3))
paste(c(1,2,3), collapse=NA)
paste('Hi!', c(1,2,3))
paste('Hi!', c(1,2,3), collapse=' ')
paste('Hi!', paste(c(1,2,3), collapse=' '))
library(restriktor) # Used for the constrained regression.
library(ggplot2)
library(gridExtra)
data <- read.csv('ss-data.csv')
# --------------- Part 1: Data Preprocessing --------------------------
recode <- function(vec, from.vals, to.vals) {
f = function(v) {
for(i in 1:length(from.vals)) {
if(v == from.vals[i]) { return(to.vals[i]) }
}
return(v)
}
return(sapply(as.vector(vec), f))
}
# Function for imputing mean of a vector to its missing values.
impute.mean <- function(vec) {
vec[is.na(vec)] = mean(vec, na.rm=TRUE)
vec
}
# Function for imputing value of 0 to each missing value in the vector.
impute.0 <- function(vec) {
vec[is.na(vec)] = 0
vec
}
# Clean and recode numeric columns.
numeric.columns <- colnames(data)[8:ncol(data)]
for(col in numeric.columns) {
data[[col]] <- as.numeric(as.character(data[[col]]))
}
imputer.f <- impute.0 # Missing value imputation function - can change if needed
# Impute missing values to component scores and print out the percent missing in each column.
for(i in 8:ncol(data)) {
message(paste('Percent Missing Values for Column ', colnames(data)[i], ': ',
round(100*(1 - (length(sort(data[[i]]))/nrow(data))), 2)))
data[[i]] <- imputer.f(data[[i]])
}
# Properly binarize binary columns.
data$Statewide.High.Priority <- as.numeric(sapply(data$Statewide.High.Priority, function(x){if(x == 'x') return(1); return(0);}))
data$District.Grant <- as.numeric(sapply(data$District.Grant, function(x){if(x == 'x') return(1); return(0);}))
fb <- subset(data, District='Fredericksburg')
nrow(fb)
fb <- subset(data, District=='Fredericksburg')
nrow(fb)
fb$Area.Type
View(fb)
colnames(fb)
fb$Access.to.Jobs
max(data$Access.to.Jobs)
max(fb$Access.to.Jobs)
aggregate(Access.to.Jobs ~ Area.type, data, FUN=mean)
aggregate(data$Access.to.Jobs ~ data$Area.type, FUN=mean)
aggregate(data$Access.to.Jobs ~ data$Area.Type, FUN=mean)
ceiling(12.3)
75/45
â™¦
bin(1:100, 10)
bincode
.bincode
.bincode(1:100, 10)
cut(1:100, 5)
range
sapply(1:100, function(x){x <= 10})
sapply(1:100, function(x){as.numeric(x <= 10)})
tabulate.by.ranges <- function(v, ranges) {
counts <- c()
ranges <- sort(ranges)
i <- 1
for(i in 1:(length(ranges) + 1)) {
if(i == 1) {
f <- function(x) {sum(sapply(v, function(y){is.numeric(y <= ranges[1])}))}
counts[1] <- f(v)
}
else if(i <= length(ranges)) {
f <- function(x) {sum(sapply(v, function(y){is.numeric((y <= ranges[i]) & (y > ranges[i-1]))}))}
counts[i] <- f(v)
}
else {
f <- function(x) {sum(sapply(v, function(y){is.numeric(y > ranges[i-1])}))}
counts[i] <- f(v)
}
}
}
tabulate.by.ranges(1:100, c(2, 30, 90))
tabulate.by.ranges <- function(v, ranges) {
counts <- c()
ranges <- sort(ranges)
i <- 1
for(i in 1:(length(ranges) + 1)) {
if(i == 1) {
f <- function(x) {sum(sapply(v, function(y){is.numeric(y <= ranges[1])}))}
counts[1] <- f(v)
}
else if(i <= length(ranges)) {
f <- function(x) {sum(sapply(v, function(y){is.numeric((y <= ranges[i]) & (y > ranges[i-1]))}))}
counts[i] <- f(v)
}
else {
f <- function(x) {sum(sapply(v, function(y){is.numeric(y > ranges[i-1])}))}
counts[i] <- f(v)
}
}
counts
}
tabulate.by.ranges(1:100, c(2, 30, 90))
1:100 <= 10
sum((1:100 >= 10) & (1:100 < 30))
tabulate.by.ranges <- function(v, ranges) {
counts <- c()
ranges <- sort(ranges)
i <- 1
for(i in 1:(length(ranges) + 1)) {
if(i == 1) {
counts[1] <- sum(v <= ranges[1])
}
else if(i <= length(ranges)) {
counts[i] <- sum((v > ranges[i-1]) & (v <= ranges[i]))
}
else {
counts[i] <- sum(v > ranges[i])
}
}
counts
}
tabulate.by.ranges(1:100, c(2, 30, 90))
tabulate.by.ranges <- function(v, ranges) {
counts <- c()
ranges <- sort(ranges)
i <- 1
for(i in 1:(length(ranges) + 1)) {
if(i == 1) {
counts[1] <- sum(v <= ranges[1])
}
else if(i <= length(ranges)) {
counts[i] <- sum((v > ranges[i - 1]) & (v <= ranges[i]))
}
else {
counts[i] <- sum(v > ranges[i - 1])
}
}
counts
}
tabulate.by.ranges(1:100, c(2, 30, 90))
tabulate.by.ranges <- function(v, ranges) {
counts <- c()
ranges <- sort(ranges)
i <- 1
for(i in 1:(length(ranges) + 1)) {
if(i == 1) {
counts[1] <- sum(v <= ranges[1])
}
else(i <= length(ranges)) {
counts[i] <- sum((v > ranges[i - 1]) & (v <= ranges[i]))
}
}
counts
}
tabulate.by.ranges <- function(v, ranges) {
counts <- c()
ranges <- sort(ranges)
i <- 1
for(i in 1:length(ranges)) {
if(i == 1) {
counts[1] <- sum(v <= ranges[1])
}
else {
counts[i] <- sum((v > ranges[i - 1]) & (v <= ranges[i]))
}
}
counts
}
tabulate.by.ranges(1:100, c(2, 30, 90))
tabulate.by.ranges(1:100, c(2, 30, 90, 100))
tabulate.by.ranges <- function(v, ranges) {
counts <- c()
ranges <- sort(ranges)
i <- 1
for(i in 1:(length(ranges) + 1)) {
if(i == 1) {
counts[1] <- sum(v <= ranges[1])
}
else if(i <= length(ranges)) {
counts[i] <- sum((v > ranges[i - 1]) & (v <= ranges[i]))
}
else {
counts[i] <- sum(v > ranges[i - 1])
}
}
counts
}
tabulate.by.ranges(1:100, c(2, 30, 90))
library(restriktor) # Used for the constrained regression.
library(ggplot2)
library(gridExtra)
data <- read.csv('ss-data.csv')
# --------------- Part 1: Data Preprocessing --------------------------
recode <- function(vec, from.vals, to.vals) {
f = function(v) {
for(i in 1:length(from.vals)) {
if(v == from.vals[i]) { return(to.vals[i]) }
}
return(v)
}
return(sapply(as.vector(vec), f))
}
# Function for imputing mean of a vector to its missing values.
impute.mean <- function(vec) {
vec[is.na(vec)] = mean(vec, na.rm=TRUE)
vec
}
# Function for imputing value of 0 to each missing value in the vector.
impute.0 <- function(vec) {
vec[is.na(vec)] = 0
vec
}
# Clean and recode numeric columns.
numeric.columns <- colnames(data)[8:ncol(data)]
for(col in numeric.columns) {
data[[col]] <- as.numeric(as.character(data[[col]]))
}
imputer.f <- impute.0 # Missing value imputation function - can change if needed
# Impute missing values to component scores and print out the percent missing in each column.
for(i in 8:ncol(data)) {
message(paste('Percent Missing Values for Column ', colnames(data)[i], ': ',
round(100*(1 - (length(sort(data[[i]]))/nrow(data))), 2)))
data[[i]] <- imputer.f(data[[i]])
}
# Properly binarize binary columns.
data$Statewide.High.Priority <- as.numeric(sapply(data$Statewide.High.Priority, function(x){if(x == 'x') return(1); return(0);}))
data$District.Grant <- as.numeric(sapply(data$District.Grant, function(x){if(x == 'x') return(1); return(0);}))
tabulate.by.ranges <- function(v, ranges) {
counts <- c()
ranges <- sort(ranges)
i <- 1
for(i in 1:(length(ranges) + 1)) {
if(i == 1) {
counts[1] <- sum(v <= ranges[1])
}
else {
counts[i] <- sum((v > ranges[i - 1]) & (v <= ranges[i]))
}
}
counts
}
tabulate.by.ranges(1:100, 10, 15, 100)
tabulate.by.ranges(1:100, c(10, 15, 100))
tabulate.by.ranges <- function(v, ranges) {
counts <- c()
ranges <- sort(ranges)
i <- 1
for(i in 1:length(ranges)) {
if(i == 1) {
counts[1] <- sum(v <= ranges[1])
}
else {
counts[i] <- sum((v > ranges[i - 1]) & (v <= ranges[i]))
}
}
counts
}
tabulate.by.ranges(1:100, c(10, 15, 100))
?t.test
z.test
1:100
se1(1, 100, by=1.3)
seq(1, 100, by=1.3)
seq(100, 1, by=-1.3)
rep(8, 20)
rep(c(1,3,5), 20)
v <- 1:99
v[100] <- NA
v
mean(v)
mean(v, na.rm=TRUE)
str.1 <- "This is some text data"
paste(str1, 'So is this')
paste(str.1, 'So is this')
message('THis is a message!')
'This is a message!'
str.vec <- c('This', 'is', 'a', 'vector', 'of', 'strings', '!')
str.vec
pnorm(1.65)
pnorm(2)
pnorm(3)
qnorm(0.9986501)
qnorm(pnorm(3))
qt(0.05, 21, lower.tail=FALSE)\
qt(0.05, 21, lower.tail=FALSE)
getwd()
dir()
setwd("C:/Users/Administrator/Desktop/R-Tutorial/data-sets")
dir()
home.prices <- read.csv('3yr-home-prices.csv')
head(home.prices)
home.prices$X2004
mean(home.prices$X2004)
mean(home.prices$X2004, na.rm=TRUE)
mean(home.prices$X2005, na.rm=TRUE)
mean(home.prices$X2006, na.rm=TRUE)
head(home.prices[X2004:X2006])
head(home.prices["X2004":"X2006"])
head(home.prices[3:5])
head(home.prices['X2004'])
head(home.prices[c('X2004', 'X2005', 'X2006')])
home.prices([1:10, 1:5])
home.prices[1:10, 1:5]
home.prices[1:10, 3:5]
home.prices[1:10, c('Metropolitan.Area', 'X2004')]
home.prices[1:10, 1:5]
home.prices[1:10, ]
home.prices[, 3:4]
home.prices[1:10, ]
home.prices[2,4]
subset(home.prices, X2004 >= 500)
subset(home.prices, X2004 <= 120 & X2006 >= 150)
